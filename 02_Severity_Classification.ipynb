{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = pd.read_csv(\"train.csv\")\n",
    "disaster_tweets = classified_df[classified_df[\"target\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only Disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Disaster Tweets: 3271\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Disaster Tweets: {len(disaster_tweets)}\")\n",
    "disaster_tweets.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new Dataset with only Disaster Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disaster = disaster_tweets.to_csv(\"disaster_tweets_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity Classification Using a Hybrid Approach (VADER+Keyword based approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VADER is fast and lightweight (works without a GPU).\n",
    "- Works well for short texts like tweets\n",
    "- Doesn't require training data - pre-built lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset = pd.read_csv('disaster_tweets_train.csv')\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Severity Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mild_keywords = [\n",
    "    \"rain\", \"wind\", \"flooded roads\", \"small fire\", \"light drizzle\", \"cloudy\", \"showers\", \"mist\", \"puddle\",\n",
    "    \"sprinkle\", \"gentle breeze\", \"fog\", \"haze\", \"overcast\", \"minor landslide\", \"light storm\", \"storm warning\",\n",
    "    \"roadblock\", \"tree fallen\", \"small accident\", \"traffic delay\", \"mild cold\", \"heatwave\", \"warm weather\",\n",
    "    \"light snow\", \"ice patches\", \"muddy roads\", \"low visibility\", \"power fluctuation\", \"communication issue\",\n",
    "    \"transportation delay\", \"evacuation alert\", \"minor injuries\", \"isolated incident\", \"waterlogging\"\n",
    "]\n",
    "\n",
    "moderate_keywords = [\n",
    "    \"damaged buildings\", \"collapsed roof\", \"power outage\", \"strong winds\", \"heavy rain\", \"thunderstorm\",\n",
    "    \"landslide\", \"mudslide\", \"flash flood\", \"bridge damage\", \"highway closure\", \"road erosion\", \"hailstorm\",\n",
    "    \"tornado warning\", \"moderate injuries\", \"partial collapse\", \"severe traffic disruption\", \"fire outbreak\",\n",
    "    \"electricity cut\", \"flooded basement\", \"forced evacuation\", \"rescue operation\", \"moderate casualties\",\n",
    "    \"hospitalization\", \"disaster relief\", \"weather advisory\", \"tropical storm\", \"communication blackout\",\n",
    "    \"food shortage\", \"water supply disruption\", \"tsunami alert\", \"gas leak\", \"volcanic ash\"\n",
    "]\n",
    "\n",
    "severe_keywords = [\n",
    "    \"earthquake\", \"tsunami\", \"destroyed\", \"many casualties\", \"collapsed buildings\", \"major landslide\",\n",
    "    \"massive flood\", \"hurricane\", \"tornado\", \"wildfire\", \"volcanic eruption\", \"severe drought\", \"famine\",\n",
    "    \"severe injuries\", \"mass evacuation\", \"missing persons\", \"death toll\", \"catastrophic event\", \"total destruction\",\n",
    "    \"chemical spill\", \"radioactive contamination\", \"oil spill\", \"airplane crash\", \"train derailment\",\n",
    "    \"terrorist attack\", \"nuclear disaster\", \"state of emergency\", \"international aid\", \"critical condition\",\n",
    "    \"uninhabitable\", \"displacement\", \"mass casualties\", \"collapsed bridge\", \"citywide blackout\",\n",
    "    \"pandemic outbreak\", \"biological hazard\", \"civil unrest\", \"armed conflict\", \"explosion\", \"war zone\",\n",
    "    \"severe fire\", \"water crisis\", \"riots\", \"looting\", \"disaster zone\", \"humanitarian crisis\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_severity(text):\n",
    "    \"\"\"Classifies severity based on keyword matching.\"\"\"\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in severe_keywords):\n",
    "        return \"Severe\"\n",
    "    elif any(word in text for word in moderate_keywords):\n",
    "        return \"Moderate\"\n",
    "    elif any(word in text for word in mild_keywords):\n",
    "        return \"Mild\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "df_dataset[\"severity_rule_based\"] = df_dataset[\"text\"].apply(classify_severity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_severity(text):\n",
    "    \"\"\"Assign severity based on sentiment scores.\"\"\"\n",
    "    score = analyzer.polarity_scores(text)[\"compound\"]\n",
    "    if score <= -0.5:\n",
    "        return \"Severe\"\n",
    "    elif -0.5 < score <= -0.2:\n",
    "        return \"Moderate\"\n",
    "    elif -0.2 < score:\n",
    "        return \"Mild\"\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply sentiment analysis severity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset[\"severity_sentiment_based\"] = df_dataset[\"text\"].apply(sentiment_severity)\n",
    "\n",
    "def final_severity(row):\n",
    "    \"\"\"Combine both approaches. If they match, return that label. Otherwise, prioritize sentiment analysis.\"\"\"\n",
    "    if row[\"severity_rule_based\"] == row[\"severity_sentiment_based\"]:\n",
    "        return row[\"severity_rule_based\"]\n",
    "    return row[\"severity_sentiment_based\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply final severity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text severity_rule_based  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...              Severe   \n",
      "1             Forest fire near La Ronge Sask. Canada             Unknown   \n",
      "2  All residents asked to 'shelter in place' are ...             Unknown   \n",
      "3  13,000 people receive #wildfires evacuation or...              Severe   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...              Severe   \n",
      "5  #RockyFire Update => California Hwy. 20 closed...              Severe   \n",
      "6  #flood #disaster Heavy rain causes flash flood...            Moderate   \n",
      "7  I'm on top of the hill and I can see a fire in...             Unknown   \n",
      "8  There's an emergency evacuation happening now ...             Unknown   \n",
      "9  I'm afraid that the tornado is coming to our a...              Severe   \n",
      "\n",
      "  severity_sentiment_based final_severity  \n",
      "0                     Mild           Mild  \n",
      "1                 Moderate       Moderate  \n",
      "2                 Moderate       Moderate  \n",
      "3                     Mild           Mild  \n",
      "4                     Mild           Mild  \n",
      "5                 Moderate       Moderate  \n",
      "6                   Severe         Severe  \n",
      "7                     Mild           Mild  \n",
      "8                 Moderate       Moderate  \n",
      "9                     Mild           Mild  \n"
     ]
    }
   ],
   "source": [
    "df_dataset[\"final_severity\"] = df_dataset.apply(final_severity, axis=1)\n",
    "print(df_dataset[[\"text\", \"severity_rule_based\", \"severity_sentiment_based\", \"final_severity\"]].head(10))\n",
    "df_dataset.to_csv(\"disaster_tweets_with_severity_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By manually checking these 25 results, we can see that keywords based + VADER approach was good but not that accurate. We need to use a transformer based model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3271 entries, 0 to 3270\n",
      "Data columns (total 8 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id                        3271 non-null   int64 \n",
      " 1   keyword                   3229 non-null   object\n",
      " 2   location                  2196 non-null   object\n",
      " 3   text                      3271 non-null   object\n",
      " 4   target                    3271 non-null   int64 \n",
      " 5   severity_sentiment_based  3271 non-null   object\n",
      " 6   severity_rule_based       3271 non-null   object\n",
      " 7   final_severity            3271 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 204.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity Classification using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_dataset = pd.read_csv('disaster_tweets_with_severity_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the transformer model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_labels = {\"Mild\": 0, \"Moderate\": 1, \"Severe\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custome Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeverityDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.texts[idx], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].squeeze(), \"attention_mask\": encoding[\"attention_mask\"].squeeze(), \"label\": torch.tensor(self.labels[idx], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_dataset[\"text\"].tolist()\n",
    "train_labels = [severity_labels[label] for label in df_dataset[\"final_severity\"].tolist()]  # Using rule-based labels as weak supervision\n",
    "\n",
    "train_dataset = SeverityDataset(train_texts, train_labels, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8008\n",
      "Epoch 2, Loss: 0.4626\n",
      "Epoch 3, Loss: 0.2803\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train_model(model, train_loader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"label\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('severity_classifier_model\\\\tokenizer_config.json',\n",
       " 'severity_classifier_model\\\\special_tokens_map.json',\n",
       " 'severity_classifier_model\\\\vocab.txt',\n",
       " 'severity_classifier_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"severity_classifier_model\")\n",
    "tokenizer.save_pretrained(\"severity_classifier_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_severity(texts, model, tokenizer):\n",
    "    model.eval()\n",
    "    encoded_inputs = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(device) for k, v in encoded_inputs.items()})\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    return [list(severity_labels.keys())[p] for p in predictions]\n",
    "\n",
    "df_dataset[\"final_severity\"] = predict_severity(df_dataset[\"text\"].tolist(), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df_dataset.to_csv(\"disaster_tweets_with_severity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text final_severity\n",
      "0   Our Deeds are the Reason of this #earthquake M...           Mild\n",
      "1              Forest fire near La Ronge Sask. Canada       Moderate\n",
      "2   All residents asked to 'shelter in place' are ...       Moderate\n",
      "3   13,000 people receive #wildfires evacuation or...           Mild\n",
      "4   Just got sent this photo from Ruby #Alaska as ...           Mild\n",
      "5   #RockyFire Update => California Hwy. 20 closed...       Moderate\n",
      "6   #flood #disaster Heavy rain causes flash flood...         Severe\n",
      "7   I'm on top of the hill and I can see a fire in...           Mild\n",
      "8   There's an emergency evacuation happening now ...       Moderate\n",
      "9   I'm afraid that the tornado is coming to our a...           Mild\n",
      "10        Three people died from the heat wave so far         Severe\n",
      "11  Haha South Tampa is getting flooded hah- WAIT ...           Mild\n",
      "12  #raining #flooding #Florida #TampaBay #Tampa 1...       Moderate\n",
      "13            #Flood in Bago Myanmar #We arrived Bago           Mild\n",
      "14  Damage to school bus on 80 in multi car crash ...         Severe\n",
      "15  @bbcmtd Wholesale Markets ablaze http://t.co/l...           Mild\n",
      "16  #AFRICANBAZE: Breaking news:Nigeria flag set a...           Mild\n",
      "17  INEC Office in Abia Set Ablaze - http://t.co/3...           Mild\n",
      "18  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...           Mild\n",
      "19  How the West was burned: Thousands of wildfire...           Mild\n",
      "20  Deputies: Man shot before Brighton home set ab...           Mild\n",
      "21  Man wife get six years jail for setting ablaze...           Mild\n",
      "22  Police: Arsonist Deliberately Set Black Church...           Mild\n",
      "23  #Kurds trampling on Turkmen flag later set it ...           Mild\n",
      "24  TRUCK ABLAZE : R21. VOORTREKKER AVE. OUTSIDE O...           Mild\n"
     ]
    }
   ],
   "source": [
    "print(df_dataset[[\"text\", \"final_severity\"]].head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test = pd.read_csv('disaster_classification_results.csv')\n",
    "df_disaster_tweets = df_test[df_test[\"predicted_label\"] == 1].reset_index(drop=True)\n",
    "df_disaster_tweets.to_csv(\"disaster_tweets_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('disaster_tweets_test.csv')\n",
    "test_texts = df_test[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>predicted_severity</th>\n",
       "      <th>disaster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>46</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London</td>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>India</td>\n",
       "      <td>Rape victim dies as she sets herself ablaze: A...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>99</td>\n",
       "      <td>accident</td>\n",
       "      <td>Homewood, PA</td>\n",
       "      <td>Accident cleared in #PaTurnpike on PATP EB bet...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111</td>\n",
       "      <td>accident</td>\n",
       "      <td>Bexhill</td>\n",
       "      <td>@Traffic_SouthE @roadpol_east Accident on A27 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>116</td>\n",
       "      <td>accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>For Legal and Medical Referral Service @1800_I...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>124</td>\n",
       "      <td>accident</td>\n",
       "      <td>All Motorways, UK</td>\n",
       "      <td>On the #M42 northbound between junctions J3 an...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>127</td>\n",
       "      <td>accident</td>\n",
       "      <td>Gresham, OR</td>\n",
       "      <td>ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>140</td>\n",
       "      <td>accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>@Calum5SOS this happened on accident but I lik...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>142</td>\n",
       "      <td>accident</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Please donate and spread the word! A training ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>169</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>Detroit, MI</td>\n",
       "      <td>'We are still living in the aftershock of Hiro...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mild</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>200</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Lake Charles, LA</td>\n",
       "      <td>When carelessness leads to an aviation acciden...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>206</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>A girl who died in an airplane accident fiftee...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>207</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>@Mintechan Hihow are you? There is Keio line o...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>214</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Eagle Pass, Texas</td>\n",
       "      <td>Mexican airplane accident in Ocampo Coahuila M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>217</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Horrible Accident  Man Died In Wings of Airpla...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>223</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>inland empire ca</td>\n",
       "      <td>@god if an accident were to happen on this air...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>224</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Muscat</td>\n",
       "      <td>Horrible Accident Man Died In Wings of Airplan...</td>\n",
       "      <td>1</td>\n",
       "      <td>Severe</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>227</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>#UPDATE: Picture from the Penn Twp. airplane a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>228</td>\n",
       "      <td>airplane accident</td>\n",
       "      <td>New York</td>\n",
       "      <td>See how a judge ruled in this 2009 accident at...</td>\n",
       "      <td>1</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id            keyword           location  \\\n",
       "0     0            Unknown            Unknown   \n",
       "1     2            Unknown            Unknown   \n",
       "2     3            Unknown            Unknown   \n",
       "3     9            Unknown            Unknown   \n",
       "4    11            Unknown            Unknown   \n",
       "5    12            Unknown            Unknown   \n",
       "6    46             ablaze             London   \n",
       "7    75             ablaze              India   \n",
       "8    99           accident       Homewood, PA   \n",
       "9   111           accident            Bexhill   \n",
       "10  116           accident            Unknown   \n",
       "11  124           accident  All Motorways, UK   \n",
       "12  127           accident        Gresham, OR   \n",
       "13  140           accident            Unknown   \n",
       "14  142           accident         Las Vegas    \n",
       "15  169         aftershock        Detroit, MI   \n",
       "16  200  airplane accident   Lake Charles, LA   \n",
       "17  206  airplane accident            Unknown   \n",
       "18  207  airplane accident            Unknown   \n",
       "19  214  airplane accident  Eagle Pass, Texas   \n",
       "20  217  airplane accident            Unknown   \n",
       "21  223  airplane accident   inland empire ca   \n",
       "22  224  airplane accident             Muscat   \n",
       "23  227  airplane accident            Unknown   \n",
       "24  228  airplane accident           New York   \n",
       "\n",
       "                                                 text  predicted_label  \\\n",
       "0                  Just happened a terrible car crash                1   \n",
       "1   Heard about #earthquake is different cities, s...                1   \n",
       "2   there is a forest fire at spot pond, geese are...                1   \n",
       "3            Apocalypse lighting. #Spokane #wildfires                1   \n",
       "4       Typhoon Soudelor kills 28 in China and Taiwan                1   \n",
       "5                  We're shaking...It's an earthquake                1   \n",
       "6   Birmingham Wholesale Market is ablaze BBC News...                1   \n",
       "7   Rape victim dies as she sets herself ablaze: A...                1   \n",
       "8   Accident cleared in #PaTurnpike on PATP EB bet...                1   \n",
       "9   @Traffic_SouthE @roadpol_east Accident on A27 ...                1   \n",
       "10  For Legal and Medical Referral Service @1800_I...                1   \n",
       "11  On the #M42 northbound between junctions J3 an...                1   \n",
       "12  ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF ...                1   \n",
       "13  @Calum5SOS this happened on accident but I lik...                1   \n",
       "14  Please donate and spread the word! A training ...                1   \n",
       "15  'We are still living in the aftershock of Hiro...                1   \n",
       "16  When carelessness leads to an aviation acciden...                1   \n",
       "17  A girl who died in an airplane accident fiftee...                1   \n",
       "18  @Mintechan Hihow are you? There is Keio line o...                1   \n",
       "19  Mexican airplane accident in Ocampo Coahuila M...                1   \n",
       "20  Horrible Accident  Man Died In Wings of Airpla...                1   \n",
       "21  @god if an accident were to happen on this air...                1   \n",
       "22  Horrible Accident Man Died In Wings of Airplan...                1   \n",
       "23  #UPDATE: Picture from the Penn Twp. airplane a...                1   \n",
       "24  See how a judge ruled in this 2009 accident at...                1   \n",
       "\n",
       "   predicted_severity  disaster_label  \n",
       "0              Severe               2  \n",
       "1                Mild               0  \n",
       "2              Severe               2  \n",
       "3                Mild               0  \n",
       "4              Severe               2  \n",
       "5                Mild               0  \n",
       "6                Mild               0  \n",
       "7              Severe               2  \n",
       "8            Moderate               1  \n",
       "9            Moderate               1  \n",
       "10           Moderate               1  \n",
       "11           Moderate               1  \n",
       "12           Moderate               1  \n",
       "13               Mild               0  \n",
       "14           Moderate               1  \n",
       "15               Mild               0  \n",
       "16             Severe               2  \n",
       "17             Severe               2  \n",
       "18           Moderate               1  \n",
       "19             Severe               2  \n",
       "20             Severe               2  \n",
       "21           Moderate               1  \n",
       "22             Severe               2  \n",
       "23           Moderate               1  \n",
       "24           Moderate               1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Saved Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_path = \"severity_classifier_model\"\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference to Predict Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    logits = outputs.logits\n",
    "    predicted_labels = torch.argmax(logits, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map Predictions to Severity Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_labels = {0: \"Mild\", 1: \"Moderate\", 2: \"Severe\"}\n",
    "df_test[\"predicted_severity\"] = [severity_labels[label] for label in predicted_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"disaster_label\"] = predicted_labels\n",
    "df_disaster_tweets = df_test[df_test[\"disaster_label\"] == 1]  \n",
    "df_disaster_tweets = df_disaster_tweets.reset_index(drop=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text predicted_severity\n",
      "0                  Just happened a terrible car crash             Severe\n",
      "1   Heard about #earthquake is different cities, s...               Mild\n",
      "2   there is a forest fire at spot pond, geese are...             Severe\n",
      "3            Apocalypse lighting. #Spokane #wildfires               Mild\n",
      "4       Typhoon Soudelor kills 28 in China and Taiwan             Severe\n",
      "5                  We're shaking...It's an earthquake               Mild\n",
      "6   Birmingham Wholesale Market is ablaze BBC News...               Mild\n",
      "7   Rape victim dies as she sets herself ablaze: A...             Severe\n",
      "8   Accident cleared in #PaTurnpike on PATP EB bet...           Moderate\n",
      "9   @Traffic_SouthE @roadpol_east Accident on A27 ...           Moderate\n",
      "10  For Legal and Medical Referral Service @1800_I...           Moderate\n",
      "11  On the #M42 northbound between junctions J3 an...           Moderate\n",
      "12  ACCIDENT - HIT AND RUN - COLD at 500 BLOCK OF ...           Moderate\n",
      "13  @Calum5SOS this happened on accident but I lik...               Mild\n",
      "14  Please donate and spread the word! A training ...           Moderate\n",
      "15  'We are still living in the aftershock of Hiro...               Mild\n",
      "16  When carelessness leads to an aviation acciden...             Severe\n",
      "17  A girl who died in an airplane accident fiftee...             Severe\n",
      "18  @Mintechan Hihow are you? There is Keio line o...           Moderate\n",
      "19  Mexican airplane accident in Ocampo Coahuila M...             Severe\n",
      "20  Horrible Accident  Man Died In Wings of Airpla...             Severe\n",
      "21  @god if an accident were to happen on this air...           Moderate\n",
      "22  Horrible Accident Man Died In Wings of Airplan...             Severe\n",
      "23  #UPDATE: Picture from the Penn Twp. airplane a...           Moderate\n",
      "24  See how a judge ruled in this 2009 accident at...           Moderate\n",
      "Predictions saved to 'disaster_tweets_with_predicted_severity.csv'.\n"
     ]
    }
   ],
   "source": [
    "print(df_test[[\"text\", \"predicted_severity\"]].head(25))  # Show first 25 predictions\n",
    "\n",
    "# Save the test dataset with predicted severity\n",
    "df_test.to_csv(\"disaster_tweets_with_predicted_severity_results.csv\", index=False)\n",
    "print(\"Predictions saved to 'disaster_tweets_with_predicted_severity.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
